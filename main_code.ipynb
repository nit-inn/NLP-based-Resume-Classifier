{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         ENSEMBLED MODEL XGBOOST AND LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment encoding has been updated in the original file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('final_test.csv')\n",
    "\n",
    "# Define the encoding function\n",
    "def linear_encode_sentiment(value):\n",
    "    if value == 'Positive':\n",
    "        return 1\n",
    "    elif value == 'Neutral':\n",
    "        return 0\n",
    "    elif value == 'Negative':\n",
    "        return -1\n",
    "    else:\n",
    "        return 0  # Default for any unexpected sentiment values\n",
    "\n",
    "# Apply linear encoding to the 'sentiment' column\n",
    "df['sentiment'] = df['sentiment'].apply(linear_encode_sentiment)\n",
    "\n",
    "# Save the updated DataFrame back to the same file\n",
    "df.to_csv('final_test.csv', index=False)\n",
    "\n",
    "print(\"The sentiment encoding has been updated in the original file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exper</th>\n",
       "      <th>incent</th>\n",
       "      <th>ticket</th>\n",
       "      <th>member</th>\n",
       "      <th>earning fm</th>\n",
       "      <th>depend</th>\n",
       "      <th>work</th>\n",
       "      <th>Graduate</th>\n",
       "      <th>language_sorted</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>CV_Score_1_to_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1.870000e+02</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>187.0</td>\n",
       "      <td>187.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.231239</td>\n",
       "      <td>8491.978610</td>\n",
       "      <td>7.173797e+05</td>\n",
       "      <td>3.368984</td>\n",
       "      <td>0.978610</td>\n",
       "      <td>2.593583</td>\n",
       "      <td>0.874332</td>\n",
       "      <td>0.695936</td>\n",
       "      <td>0.631016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.254587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.274825</td>\n",
       "      <td>5207.902374</td>\n",
       "      <td>6.916836e+05</td>\n",
       "      <td>1.370857</td>\n",
       "      <td>1.140445</td>\n",
       "      <td>1.423898</td>\n",
       "      <td>0.330337</td>\n",
       "      <td>0.379070</td>\n",
       "      <td>0.760377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>1.250000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.923092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.166667</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>3.500000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.166833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.832908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.610969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            exper        incent        ticket      member  earning fm  \\\n",
       "count  187.000000    187.000000  1.870000e+02  187.000000  187.000000   \n",
       "mean     4.231239   8491.978610  7.173797e+05    3.368984    0.978610   \n",
       "std      4.274825   5207.902374  6.916836e+05    1.370857    1.140445   \n",
       "min      0.000000      0.000000  0.000000e+00    0.000000    0.000000   \n",
       "25%      2.000000   5000.000000  1.250000e+05    3.000000    0.000000   \n",
       "50%      3.166667   8000.000000  3.500000e+05    3.000000    1.000000   \n",
       "75%      6.000000  15000.000000  1.200000e+06    5.000000    1.000000   \n",
       "max     40.000000  15000.000000  2.000000e+06    5.000000    5.000000   \n",
       "\n",
       "           depend        work    Graduate  language_sorted  sentiment  \\\n",
       "count  187.000000  187.000000  187.000000       187.000000      187.0   \n",
       "mean     2.593583    0.874332    0.695936         0.631016        0.0   \n",
       "std      1.423898    0.330337    0.379070         0.760377        0.0   \n",
       "min      0.000000    0.000000    0.000000        -1.000000        0.0   \n",
       "25%      2.000000    1.000000    0.430000         1.000000        0.0   \n",
       "50%      3.000000    1.000000    1.000000         1.000000        0.0   \n",
       "75%      4.000000    1.000000    1.000000         1.000000        0.0   \n",
       "max      7.000000    1.000000    1.000000         1.000000        0.0   \n",
       "\n",
       "       CV_Score_1_to_5  \n",
       "count       187.000000  \n",
       "mean          3.254587  \n",
       "std           0.630898  \n",
       "min           1.000000  \n",
       "25%           2.923092  \n",
       "50%           3.166833  \n",
       "75%           3.832908  \n",
       "max           4.610969  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    187\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.18.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "Requirement already satisfied: tensorflow in c:\\users\\nityam\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
      "Installing collected packages: tensorflow\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "  Rolling back uninstall of tensorflow\n",
      "  Moving to c:\\users\\nityam\\anaconda3\\lib\\site-packages\\tensorflow-2.18.0.dist-info\\\n",
      "   from C:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\~ensorflow-2.18.0.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Nityam\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\compiler\\\\tf2tensorrt\\\\_pywrap_py_utils.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the current TensorFlow version\n",
    "!pip show tensorflow\n",
    "\n",
    "# Upgrade TensorFlow if needed\n",
    "!pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                 ANN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('final_train.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['CandidateID', 'detected_languages','sentiment'])\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X = data.drop(columns=['Performance'])\n",
    "y = data['Performance']\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Standardize features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the Artificial Neural Network (ANN)\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer (binary classification)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"ANN Accuracy: {accuracy}\")\n",
    "\n",
    "# Load and preprocess the test data\n",
    "dfg = pd.read_csv('final_test.csv')\n",
    "\n",
    "# Define the features dynamically\n",
    "train_features = X.columns.tolist()\n",
    "test_features = dfg.columns.tolist()\n",
    "\n",
    "# Ensure test features match train features (reorder if necessary)\n",
    "common_features = list(set(train_features) & set(test_features))\n",
    "missing_features = list(set(train_features) - set(test_features))\n",
    "\n",
    "# Add missing features as zero-filled columns in test data\n",
    "for feature in missing_features:\n",
    "    dfg[feature] = 0\n",
    "\n",
    "# Reorder test features to match train feature order\n",
    "dfg = dfg[train_features]\n",
    "\n",
    "# Impute missing values and scale test data\n",
    "ll_test_imputed = imputer.transform(dfg)\n",
    "ll_test_scaled = scaler.transform(ll_test_imputed)\n",
    "\n",
    "# Predict using the trained ANN\n",
    "Y_pred_ANN = (model.predict(ll_test_scaled) > 0.5).astype(\"int32\")\n",
    "print(Y_pred_ANN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       " 1    704\n",
       "-1     22\n",
       " 0     19\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(745, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.610738255033557\n",
      "[1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1\n",
      " 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('final_train.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['CandidateID', 'detected_languages','sentiment','language_sorted'])\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data.drop(columns=['Performance'])\n",
    "y = data['Performance']\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Standardize features (optional for XGBoost)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost Classifier with parameters for slight overfitting\n",
    "xgb_model = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    max_depth=15,               # Increase tree depth\n",
    "    learning_rate=0.01,         # Slightly higher learning rate\n",
    "    n_estimators=300,          # Increase number of estimators\n",
    "    subsample=0.9,             # Use more data for each tree\n",
    "    colsample_bytree=0.9,      # Use more features for each tree\n",
    "    reg_alpha=0.1,             # Reduce L1 regularization\n",
    "    reg_lambda=0.1             # Reduce L2 regularization\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Accuracy: {accuracy}\")\n",
    "\n",
    "# Load and preprocess the test data\n",
    "dfg = pd.read_csv('final_test.csv')\n",
    "\n",
    "# Define the features dynamically\n",
    "train_features = X.columns.tolist()\n",
    "test_features = dfg.columns.tolist()\n",
    "\n",
    "# Ensure test features match train features (reorder if necessary)\n",
    "common_features = list(set(train_features) & set(test_features))\n",
    "missing_features = list(set(train_features) - set(test_features))\n",
    "\n",
    "# Add missing features as zero-filled columns in test data\n",
    "for feature in missing_features:\n",
    "    dfg[feature] = 0\n",
    "\n",
    "# Reorder test features to match train feature order\n",
    "dfg = dfg[train_features]\n",
    "\n",
    "# Impute missing values and scale test data\n",
    "ll_test_imputed = imputer.transform(dfg)\n",
    "ll_test_scaled = scaler.transform(ll_test_imputed)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "Y_pred_XGB = xgb_model.predict(ll_test_scaled)\n",
    "print(Y_pred_XGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_index], y\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Train the ensemble model\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m ensemble_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Predict and evaluate on train data\u001b[39;00m\n\u001b[0;32m     58\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m ensemble_model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:419\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, transformed_y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get common fit operations.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     names, clfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_estimators()\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:233\u001b[0m, in \u001b[0;36m_BaseHeterogeneousEnsemble._validate_estimators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m is_estimator_type \u001b[38;5;241m=\u001b[39m is_classifier \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m is_regressor\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m estimators:\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_estimator_type(est):\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe estimator \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m should be a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    236\u001b[0m                 est\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_estimator_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m    237\u001b[0m             )\n\u001b[0;32m    238\u001b[0m         )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m names, estimators\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1237\u001b[0m, in \u001b[0;36mis_classifier\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mstack()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1234\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_estimator_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_tags(estimator)\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_tags.py:430\u001b[0m, in \u001b[0;36mget_tags\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39mmro()):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[1;32m--> 430\u001b[0m         sklearn_tags_provider[klass] \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39m__sklearn_tags__(estimator)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    431\u001b[0m         class_order\u001b[38;5;241m.\u001b[39mappend(klass)\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "File \u001b[1;32mc:\\Users\\Nityam\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:540\u001b[0m, in \u001b[0;36mClassifierMixin.__sklearn_tags__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__sklearn_tags__()\n\u001b[0;32m    541\u001b[0m     tags\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m     tags\u001b[38;5;241m.\u001b[39mclassifier_tags \u001b[38;5;241m=\u001b[39m ClassifierTags()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\nityam\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\nityam\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load the training dataset\n",
    "data = pd.read_csv('final_train.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['CandidateID', 'detected_languages'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['Performance'])\n",
    "y = data['Performance']\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Initialize Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "lgbm_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgbm', lgbm_model)\n",
    "], voting='hard')\n",
    "\n",
    "# Lists to store accuracies for each fold\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the ensemble model\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate on train data\n",
    "    y_pred_train = ensemble_model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "    # Predict and evaluate on test data\n",
    "    y_pred_test = ensemble_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    # Store accuracies\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Print the average accuracies over all folds\n",
    "print(f\"Average Train Accuracy: {np.mean(train_accuracies)}\")\n",
    "print(f\"Average Test Accuracy: {np.mean(test_accuracies)}\")\n",
    "\n",
    "# Now, make predictions on new data using the trained ensemble model\n",
    "dfg = pd.read_csv('final_test.csv')\n",
    "features = ['exper', 'incent', 'ticket', 'member', 'earning fm', 'depend', 'work', 'Graduate','sentiment', 'language_sorted' ]\n",
    "\n",
    "# Extract the relevant features for the test set\n",
    "ll_test = dfg[features]\n",
    "\n",
    "# Ensure feature alignment between training and test data\n",
    "ll_test = ll_test.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Impute missing values and scale test data\n",
    "ll_test_imputed = imputer.transform(ll_test)\n",
    "ll_test_scaled = scaler.transform(ll_test_imputed)\n",
    "\n",
    "# Predict using the trained ensemble model\n",
    "Y_pred_ensemble = ensemble_model.predict(ll_test_scaled)\n",
    "print(\"Predictions on Test Data:\", Y_pred_ensemble)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
